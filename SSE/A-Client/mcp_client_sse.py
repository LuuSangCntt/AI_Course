import streamlit as st
import asyncio
import json
from mcp import ClientSession
from mcp.client.sse import sse_client
from openai import OpenAI
from contextlib import AsyncExitStack

# --- C·∫§U H√åNH ---
with open("SSE\A-Client\servers_config.json", "r") as f:
    CONFIG = json.load(f)

llm_client = OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")

st.set_page_config(page_title="Multi-Server AI Agent", layout="wide")
st.title("üöÄ MCP Multi-Server Agent (with Debug Log)")
# 1. KH·ªûI T·∫†O K·∫æT N·ªêI (Ch·ªâ ch·∫°y m·ªôt l·∫ßn khi load app)
if "mcp_data" not in st.session_state:
    st.session_state.mcp_data = {"tools": [], "active_sessions": {}, "status": {}}

async def init_mcp_servers():
    """H√†m n√†y ch·ªâ ch·∫°y 1 l·∫ßn ƒë·ªÉ k·∫øt n·ªëi t·∫•t c·∫£ server"""
    st.session_state.mcp_data["tools"] = []
    
    # Ch√∫ng ta d√πng m·ªôt stack to√†n c·ª•c ƒë·ªÉ gi·ªØ k·∫øt n·ªëi kh√¥ng b·ªã ƒë√≥ng
    stack = AsyncExitStack()
    st.session_state.stack = stack 

    for s_cfg in CONFIG["mcp_servers"]:
        if not s_cfg.get("enabled", True): continue
        try:
            context = await stack.enter_async_context(sse_client(s_cfg["url"]))
            read, write = context
            session = await stack.enter_async_context(ClientSession(read, write))
            await session.initialize()
            
            tools_resp = await session.list_tools()
            for t in tools_resp.tools:
                st.session_state.mcp_data["tools"].append({
                    "type": "function",
                    "function": {
                        "name": t.name,
                        "description": f"[{s_cfg['name']}] {t.description}",
                        "parameters": t.inputSchema
                    },
                    "server_session": session
                })
            st.session_state.mcp_data["status"][s_cfg['name']] = "‚úÖ Online"
        except Exception as e:
            st.session_state.mcp_data["status"][s_cfg['name']] = f"‚ùå Offline: {str(e)}"

# N√∫t b·∫•m ƒë·ªÉ k·∫øt n·ªëi l·∫°i th·ªß c√¥ng n·∫øu c·∫ßn
if st.sidebar.button("üîÑ Refresh All MCP Connections"):
    asyncio.run(init_mcp_servers())

# Hi·ªÉn th·ªã tr·∫°ng th√°i server ·ªü sidebar cho g·ªçn
for name, stat in st.session_state.mcp_data["status"].items():
    st.sidebar.write(f"**{name}**: {stat}")

# --- LOGIC CHAT ---
async def run_agent(user_input):
    all_tools = st.session_state.mcp_data["tools"]
    if not all_tools:
        return "Ch∆∞a c√≥ server n√†o online. Vui l√≤ng ki·ªÉm tra sidebar."

    openai_tools = [{k: v for k, v in t.items() if k != 'server_session'} for t in all_tools]
    messages = [{"role": "user", "content": user_input}]
    
    response = llm_client.chat.completions.create(
        model="qwen3-14b",
        messages=messages,
        tools=openai_tools
    )
    
    ai_msg = response.choices[0].message
    if ai_msg.tool_calls:
        messages.append(ai_msg)
        for tool_call in ai_msg.tool_calls:
            name = tool_call.function.name
            args = json.loads(tool_call.function.arguments)
            target_tool = next(t for t in all_tools if t["function"]["name"] == name)
            
            # Th·ª±c thi tool tr√™n session ƒë√£ m·ªü s·∫µn
            result = await target_tool["server_session"].call_tool(name, args)
            messages.append({"role": "tool", "tool_call_id": tool_call.id, "name": name, "content": str(result.content)})

        final_res = llm_client.chat.completions.create(model="qwen3-14b", messages=messages)
        return final_res.choices[0].message.content
    return ai_msg.content
if "messages" not in st.session_state:
    st.session_state.messages = []

# Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])


if prompt := st.chat_input("H·ªèi t√¥i v·ªÅ to√°n ho·∫∑c database..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        response_text = asyncio.run(run_agent(prompt))
        st.markdown(response_text)
        st.session_state.messages.append({"role": "assistant", "content": response_text})